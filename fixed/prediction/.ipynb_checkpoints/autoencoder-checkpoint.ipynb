{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, LSTM, Activation, TimeDistributed, RepeatVector, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# set GPU memory\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 93)"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/home/lab606a/ML/trajectories/fixed/classification/Lebeled_by_Kmeans/dataset20200320.csv', header=None)\n",
    "dataset = dataset.fillna(0)\n",
    "dataset = np.array(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.reshape(dataset.shape[0], int(dataset.shape[1]/3), 3)\n",
    "#dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(93,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = LSTM(512, activation='linear', return_sequences=True)(input_layer)\n",
    "#encoder = LSTM(128, activation='linear', return_sequences=True)(encoder)\n",
    "#encoder = LSTM(32, activation='linear')(encoder)\n",
    "\n",
    "encoded = Dense(64, activation='linear')(input_layer)\n",
    "encoded = Dense(16, activation='linear')(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder = RepeatVector(dataset.shape[1])(encoder)\n",
    "#decoder = LSTM(32, activation='linear', return_sequences=True)(decoder)\n",
    "#decoder = LSTM(512, activation='linear', return_sequences=True)(decoder)\n",
    "#decoder = LSTM(3, activation='linear', return_sequences=True)(decoder)\n",
    "#decoder = TimeDistributed(Dense(3))(decoder)\n",
    "\n",
    "decoded = Dense(64, activation='linear')(encoded)\n",
    "decoded = Dense(93, activation='linear')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_layer, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_75 (InputLayer)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 64)                6016      \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 93)                6045      \n",
      "=================================================================\n",
      "Total params: 14,189\n",
      "Trainable params: 14,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AutoEncoder = Model(input_layer, decoded)\n",
    "AutoEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_120/BiasAdd:0' shape=(?, 16) dtype=float32>"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoEncoder.layers[2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_input = Input(shape=(16,))\n",
    "#decoder_layer = AutoEncoder.layers[-1]\n",
    "#decoded_layer = Dense(64, activation='linear')(decoded_input)\n",
    "decoded_layer = AutoEncoder.layers[3](decoded_input)\n",
    "decoded_layer = AutoEncoder.layers[4](decoded_layer)\n",
    "#ecoded_layer = Dense(93, activation='linear')(decoded_input)\n",
    "decoder = Model(decoded_input, decoded_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder = Model(input_layer, encoder)\n",
    "#Encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 51.5151 - acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 34.0764 - acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 21.2393 - acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 16.1474 - acc: 0.0017\n",
      "Epoch 5/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 13.7168 - acc: 0.0032\n",
      "Epoch 6/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 12.0232 - acc: 0.4443\n",
      "Epoch 7/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 10.6882 - acc: 0.9577\n",
      "Epoch 8/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 9.3839 - acc: 0.8607\n",
      "Epoch 9/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 8.2164 - acc: 0.7285\n",
      "Epoch 10/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 7.3084 - acc: 0.7940\n",
      "Epoch 11/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 6.7054 - acc: 0.9212\n",
      "Epoch 12/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 6.3320 - acc: 0.9595\n",
      "Epoch 13/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 6.0535 - acc: 0.9770\n",
      "Epoch 14/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 5.8643 - acc: 0.9960\n",
      "Epoch 15/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 5.6531 - acc: 0.9958\n",
      "Epoch 16/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 5.4907 - acc: 0.9953\n",
      "Epoch 17/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 5.3243 - acc: 0.9965\n",
      "Epoch 18/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 5.1603 - acc: 0.9968\n",
      "Epoch 19/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 4.9989 - acc: 0.9968\n",
      "Epoch 20/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 4.8354 - acc: 0.9965\n",
      "Epoch 21/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 4.6912 - acc: 0.9965\n",
      "Epoch 22/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 4.6491 - acc: 0.9968\n",
      "Epoch 23/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 4.4902 - acc: 0.9968\n",
      "Epoch 24/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 4.3989 - acc: 0.9968\n",
      "Epoch 25/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 4.3451 - acc: 0.9968\n",
      "Epoch 26/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 4.2990 - acc: 0.9968\n",
      "Epoch 27/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 4.2686 - acc: 0.9968\n",
      "Epoch 28/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 4.1789 - acc: 0.9968\n",
      "Epoch 29/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 4.1381 - acc: 0.9968\n",
      "Epoch 30/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 4.1084 - acc: 0.9968\n",
      "Epoch 31/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 4.0802 - acc: 0.9968\n",
      "Epoch 32/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 4.0143 - acc: 0.9968\n",
      "Epoch 33/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.9827 - acc: 0.9968\n",
      "Epoch 34/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.9513 - acc: 0.9968\n",
      "Epoch 35/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.9056 - acc: 0.9968\n",
      "Epoch 36/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 3.9041 - acc: 0.9968\n",
      "Epoch 37/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.8394 - acc: 0.9968\n",
      "Epoch 38/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 3.7938 - acc: 0.9968\n",
      "Epoch 39/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.7342 - acc: 0.9968\n",
      "Epoch 40/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 3.7122 - acc: 0.9968\n",
      "Epoch 41/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.6773 - acc: 0.9968\n",
      "Epoch 42/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.6573 - acc: 0.9968\n",
      "Epoch 43/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.6411 - acc: 0.9968\n",
      "Epoch 44/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.5657 - acc: 0.9968\n",
      "Epoch 45/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.5453 - acc: 0.9968\n",
      "Epoch 46/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.4939 - acc: 0.9968\n",
      "Epoch 47/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.4946 - acc: 0.9968\n",
      "Epoch 48/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.4787 - acc: 0.9968\n",
      "Epoch 49/200\n",
      "4000/4000 [==============================] - 0s 9us/step - loss: 3.4527 - acc: 0.9968\n",
      "Epoch 50/200\n",
      "4000/4000 [==============================] - 0s 9us/step - loss: 3.3982 - acc: 0.9968\n",
      "Epoch 51/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.3815 - acc: 0.9968\n",
      "Epoch 52/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.3554 - acc: 0.9968\n",
      "Epoch 53/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.3699 - acc: 0.9968\n",
      "Epoch 54/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.3288 - acc: 0.9968\n",
      "Epoch 55/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.2695 - acc: 0.9968\n",
      "Epoch 56/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 3.2764 - acc: 0.9968\n",
      "Epoch 57/200\n",
      "4000/4000 [==============================] - 0s 10us/step - loss: 3.2583 - acc: 0.9968\n",
      "Epoch 58/200\n",
      "4000/4000 [==============================] - 0s 9us/step - loss: 3.2388 - acc: 0.9968\n",
      "Epoch 59/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 3.2001 - acc: 0.9968\n",
      "Epoch 60/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.2140 - acc: 0.9968\n",
      "Epoch 61/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.1628 - acc: 0.9968\n",
      "Epoch 62/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.1503 - acc: 0.9968\n",
      "Epoch 63/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.1250 - acc: 0.9968\n",
      "Epoch 64/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.1099 - acc: 0.9968\n",
      "Epoch 65/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.0833 - acc: 0.9968\n",
      "Epoch 66/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.0736 - acc: 0.9968\n",
      "Epoch 67/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.0463 - acc: 0.9968\n",
      "Epoch 68/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 3.0269 - acc: 0.9968\n",
      "Epoch 69/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.9938 - acc: 0.9968\n",
      "Epoch 70/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.9872 - acc: 0.9968\n",
      "Epoch 71/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.9667 - acc: 0.9968\n",
      "Epoch 72/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.9173 - acc: 0.9968\n",
      "Epoch 73/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.9226 - acc: 0.9968\n",
      "Epoch 74/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.9011 - acc: 0.9968\n",
      "Epoch 75/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.8844 - acc: 0.9968\n",
      "Epoch 76/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.8493 - acc: 0.9968\n",
      "Epoch 77/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.8292 - acc: 0.9968\n",
      "Epoch 78/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.8136 - acc: 0.9968\n",
      "Epoch 79/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.7928 - acc: 0.9968\n",
      "Epoch 80/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 2.7784 - acc: 0.9968\n",
      "Epoch 81/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.7574 - acc: 0.9968\n",
      "Epoch 82/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.7651 - acc: 0.9968\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.7292 - acc: 0.9968\n",
      "Epoch 84/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.7210 - acc: 0.9968\n",
      "Epoch 85/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.7199 - acc: 0.9968\n",
      "Epoch 86/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.6668 - acc: 0.9968\n",
      "Epoch 87/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.6517 - acc: 0.9968\n",
      "Epoch 88/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.6662 - acc: 0.9968\n",
      "Epoch 89/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 2.6569 - acc: 0.9968\n",
      "Epoch 90/200\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 2.6286 - acc: 0.9968\n",
      "Epoch 91/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 2.6098 - acc: 0.9968\n",
      "Epoch 92/200\n",
      "4000/4000 [==============================] - 0s 7us/step - loss: 2.5973 - acc: 0.9968\n",
      "Epoch 93/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 2.5933 - acc: 0.9968\n",
      "Epoch 94/200\n",
      "4000/4000 [==============================] - 0s 8us/step - loss: 2.5596 - acc: 0.9968\n",
      "Epoch 95/200\n",
      "4000/4000 [==============================] - 0s 10us/step - loss: 2.5510 - acc: 0.9968\n",
      "Epoch 96/200\n",
      "4000/4000 [==============================] - 0s 10us/step - loss: 2.5535 - acc: 0.9968\n",
      "Epoch 97/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.5241 - acc: 0.9968\n",
      "Epoch 98/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.5115 - acc: 0.9968\n",
      "Epoch 99/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.5240 - acc: 0.9968\n",
      "Epoch 100/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.4996 - acc: 0.9968\n",
      "Epoch 101/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.4937 - acc: 0.9968\n",
      "Epoch 102/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.4879 - acc: 0.9968\n",
      "Epoch 103/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.4549 - acc: 0.9968\n",
      "Epoch 104/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.4637 - acc: 0.9968\n",
      "Epoch 105/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.4454 - acc: 0.9968\n",
      "Epoch 106/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.4367 - acc: 0.9968\n",
      "Epoch 107/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.4257 - acc: 0.9968\n",
      "Epoch 108/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.3950 - acc: 0.9968\n",
      "Epoch 109/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.3796 - acc: 0.9968\n",
      "Epoch 110/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.3880 - acc: 0.9968\n",
      "Epoch 111/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.3699 - acc: 0.9968\n",
      "Epoch 112/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.4047 - acc: 0.9968\n",
      "Epoch 113/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.3429 - acc: 0.9968\n",
      "Epoch 114/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.3684 - acc: 0.9968\n",
      "Epoch 115/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.3724 - acc: 0.9968\n",
      "Epoch 116/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.3342 - acc: 0.9968\n",
      "Epoch 117/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.3320 - acc: 0.9968\n",
      "Epoch 118/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.3211 - acc: 0.9968\n",
      "Epoch 119/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.3275 - acc: 0.9968\n",
      "Epoch 120/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.3301 - acc: 0.9968\n",
      "Epoch 121/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.3194 - acc: 0.9968\n",
      "Epoch 122/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.3092 - acc: 0.9968\n",
      "Epoch 123/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.3083 - acc: 0.9968\n",
      "Epoch 124/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.2890 - acc: 0.9968\n",
      "Epoch 125/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.2798 - acc: 0.9968\n",
      "Epoch 126/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.2748 - acc: 0.9968\n",
      "Epoch 127/200\n",
      "4000/4000 [==============================] - 0s 12us/step - loss: 2.2652 - acc: 0.9968\n",
      "Epoch 128/200\n",
      "4000/4000 [==============================] - 0s 13us/step - loss: 2.2746 - acc: 0.9968\n",
      "Epoch 129/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.2603 - acc: 0.9968\n",
      "Epoch 130/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2461 - acc: 0.9968\n",
      "Epoch 131/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.2619 - acc: 0.9968\n",
      "Epoch 132/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2647 - acc: 0.9968\n",
      "Epoch 133/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2677 - acc: 0.9968\n",
      "Epoch 134/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2387 - acc: 0.9968\n",
      "Epoch 135/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2413 - acc: 0.9968\n",
      "Epoch 136/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2205 - acc: 0.9968\n",
      "Epoch 137/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2329 - acc: 0.9968\n",
      "Epoch 138/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2291 - acc: 0.9968\n",
      "Epoch 139/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2375 - acc: 0.9968\n",
      "Epoch 140/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2489 - acc: 0.9968\n",
      "Epoch 141/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2208 - acc: 0.9968\n",
      "Epoch 142/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2248 - acc: 0.9968\n",
      "Epoch 143/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.2244 - acc: 0.9968\n",
      "Epoch 144/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2258 - acc: 0.9968\n",
      "Epoch 145/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.2094 - acc: 0.9968\n",
      "Epoch 146/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1914 - acc: 0.9968\n",
      "Epoch 147/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.1966 - acc: 0.9968\n",
      "Epoch 148/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1867 - acc: 0.9968\n",
      "Epoch 149/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1870 - acc: 0.9968\n",
      "Epoch 150/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.2035 - acc: 0.9968\n",
      "Epoch 151/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.2052 - acc: 0.9968\n",
      "Epoch 152/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1918 - acc: 0.9968\n",
      "Epoch 153/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.1772 - acc: 0.9968\n",
      "Epoch 154/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.1632 - acc: 0.9968\n",
      "Epoch 155/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1579 - acc: 0.9968\n",
      "Epoch 156/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1795 - acc: 0.9968\n",
      "Epoch 157/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.1453 - acc: 0.9968\n",
      "Epoch 158/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1647 - acc: 0.9968\n",
      "Epoch 159/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1529 - acc: 0.9968\n",
      "Epoch 160/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.1609 - acc: 0.9968\n",
      "Epoch 161/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1657 - acc: 0.9968\n",
      "Epoch 162/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1539 - acc: 0.9968\n",
      "Epoch 163/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.1490 - acc: 0.9968\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.1716 - acc: 0.9968\n",
      "Epoch 165/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.1578 - acc: 0.9968\n",
      "Epoch 166/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1469 - acc: 0.9968\n",
      "Epoch 167/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.1414 - acc: 0.9968\n",
      "Epoch 168/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.1239 - acc: 0.9968\n",
      "Epoch 169/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1369 - acc: 0.9968\n",
      "Epoch 170/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.1328 - acc: 0.9968\n",
      "Epoch 171/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.1334 - acc: 0.9968\n",
      "Epoch 172/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1498 - acc: 0.9968\n",
      "Epoch 173/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1248 - acc: 0.9968\n",
      "Epoch 174/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.1344 - acc: 0.9968\n",
      "Epoch 175/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1321 - acc: 0.9968\n",
      "Epoch 176/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.1143 - acc: 0.9968\n",
      "Epoch 177/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.1300 - acc: 0.9968\n",
      "Epoch 178/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.1298 - acc: 0.9968\n",
      "Epoch 179/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1050 - acc: 0.9968\n",
      "Epoch 180/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1247 - acc: 0.9968\n",
      "Epoch 181/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1204 - acc: 0.9968\n",
      "Epoch 182/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1031 - acc: 0.9968\n",
      "Epoch 183/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.1085 - acc: 0.9968\n",
      "Epoch 184/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.0956 - acc: 0.9968\n",
      "Epoch 185/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.0974 - acc: 0.9968\n",
      "Epoch 186/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1011 - acc: 0.9968\n",
      "Epoch 187/200\n",
      "4000/4000 [==============================] - 0s 14us/step - loss: 2.0962 - acc: 0.9968\n",
      "Epoch 188/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1009 - acc: 0.9968\n",
      "Epoch 189/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.0925 - acc: 0.9968\n",
      "Epoch 190/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.0936 - acc: 0.9968\n",
      "Epoch 191/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.1092 - acc: 0.9968\n",
      "Epoch 192/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.1019 - acc: 0.9968\n",
      "Epoch 193/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.0957 - acc: 0.9968\n",
      "Epoch 194/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.0653 - acc: 0.9968\n",
      "Epoch 195/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.0634 - acc: 0.9968\n",
      "Epoch 196/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.0874 - acc: 0.9968\n",
      "Epoch 197/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.0881 - acc: 0.9968\n",
      "Epoch 198/200\n",
      "4000/4000 [==============================] - 0s 17us/step - loss: 2.0694 - acc: 0.9968\n",
      "Epoch 199/200\n",
      "4000/4000 [==============================] - 0s 16us/step - loss: 2.0625 - acc: 0.9968\n",
      "Epoch 200/200\n",
      "4000/4000 [==============================] - 0s 15us/step - loss: 2.0595 - acc: 0.9968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15945bb278>"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoEncoder.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "AutoEncoder.fit(dataset, dataset, batch_size=400, epochs=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 93)"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = dataset[2,:]\n",
    "#x_test = x_test.reshape(1,x_test.shape[0],x_test.shape[1])\n",
    "x_test = x_test.reshape(1, x_test.shape[0])\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 76.4281 , 281.82   ,  46.1174 ,  72.3619 , 265.623  ,  49.692  ,\n",
       "         68.8458 , 245.804  ,  52.8224 ,  65.3813 , 228.59   ,  54.4357 ,\n",
       "         62.4148 , 211.559  ,  55.2543 ,  59.4883 , 194.715  ,  54.9735 ,\n",
       "         56.8059 , 178.12   ,  53.5257 ,  54.3041 , 162.602  ,  50.7069 ,\n",
       "         52.0143 , 147.223  ,  47.2051 ,  50.2217 , 130.037  ,  42.895  ,\n",
       "         48.5502 , 113.953  ,  37.4957 ,  46.8268 ,  98.7239 ,  31.1585 ,\n",
       "         45.2417 ,  83.7062 ,  23.6844 ,  42.8338 ,  70.2375 ,  10.7569 ,\n",
       "         41.8368 ,  55.0491 ,   1.438  ,  42.1877 ,  42.9471 ,   9.63741,\n",
       "         42.4645 ,  31.5497 ,  17.3978 ,  43.3453 ,  16.7613 ,  25.5065 ,\n",
       "         43.6907 ,   6.17753,  30.3264 ,  44.3477 ,  -5.16837,  34.5074 ,\n",
       "         45.0033 , -16.0711 ,  37.5376 ,  45.6844 , -26.8125 ,  39.4341 ,\n",
       "         46.3045 , -37.4043 ,  40.3361 ,  47.1156 , -47.8966 ,  40.1242 ,\n",
       "         47.7433 , -57.8115 ,  38.4804 ,   0.     ,   0.     ,   0.     ,\n",
       "          0.     ,   0.     ,   0.     ,   0.     ,   0.     ,   0.     ,\n",
       "          0.     ,   0.     ,   0.     ,   0.     ,   0.     ,   0.     ,\n",
       "          0.     ,   0.     ,   0.     ]])"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 74.62439   , 284.0131    ,  46.328217  ,  71.07204   ,\n",
       "        265.54517   ,  50.942085  ,  68.92918   , 244.91026   ,\n",
       "         53.66872   ,  64.48583   , 228.74762   ,  55.284595  ,\n",
       "         61.428783  , 211.33893   ,  55.70596   ,  59.221806  ,\n",
       "        195.37523   ,  55.38632   ,  56.389847  , 178.2012    ,\n",
       "         53.140827  ,  54.19086   , 160.95865   ,  51.163574  ,\n",
       "         52.034855  , 144.65088   ,  48.261894  ,  49.783356  ,\n",
       "        128.7273    ,  42.65701   ,  48.527405  , 113.94929   ,\n",
       "         37.81866   ,  46.576027  ,  97.84213   ,  29.910824  ,\n",
       "         44.517616  ,  83.33585   ,  23.233818  ,  43.530956  ,\n",
       "         68.86761   ,  12.159484  ,  42.34217   ,  54.943527  ,\n",
       "          5.344706  ,  41.65416   ,  43.040413  ,  10.437077  ,\n",
       "         41.128445  ,  32.20306   ,  17.98311   ,  40.29883   ,\n",
       "         19.966682  ,  24.278229  ,  39.657093  ,   9.352381  ,\n",
       "         30.136898  ,  41.438023  ,  -2.884353  ,  34.041355  ,\n",
       "         43.836857  , -15.287368  ,  37.65228   ,  43.541378  ,\n",
       "        -25.396862  ,  40.32444   ,  50.970123  , -31.854712  ,\n",
       "         36.370705  ,  50.96266   , -40.453888  ,  34.16218   ,\n",
       "         46.852497  , -38.145924  ,  25.732567  ,  35.649475  ,\n",
       "        -27.281328  ,  12.667004  ,  -5.496917  ,   2.946937  ,\n",
       "         -1.8291466 ,  -1.047319  ,   0.88303155,  -0.838334  ,\n",
       "          0.5851649 ,  -0.0396621 ,  -0.04110734,  -0.08073356,\n",
       "          0.06935974,  -0.6384577 ,  -0.6555605 ,   0.09905431,\n",
       "          0.0307072 ]], dtype=float32)"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = AutoEncoder.predict(x_test)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.2620552,   56.249687 ,  -94.9192   ,   45.662014 ,\n",
       "        -230.66032  ,   80.99495  ,  102.928894 ,  140.8034   ,\n",
       "          80.20187  ,   82.66315  ,  124.91931  , -208.93039  ,\n",
       "          77.70047  ,  109.801094 ,   16.755114 ,   32.51221  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = encoder.predict(x_test)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 74.62439   , 284.0131    ,  46.328217  ,  71.07204   ,\n",
       "        265.54517   ,  50.942085  ,  68.92918   , 244.91026   ,\n",
       "         53.66872   ,  64.48583   , 228.74762   ,  55.284595  ,\n",
       "         61.428783  , 211.33893   ,  55.70596   ,  59.221806  ,\n",
       "        195.37523   ,  55.38632   ,  56.389847  , 178.2012    ,\n",
       "         53.140827  ,  54.19086   , 160.95865   ,  51.163574  ,\n",
       "         52.034855  , 144.65088   ,  48.261894  ,  49.783356  ,\n",
       "        128.7273    ,  42.65701   ,  48.527405  , 113.94929   ,\n",
       "         37.81866   ,  46.576027  ,  97.84213   ,  29.910824  ,\n",
       "         44.517616  ,  83.33585   ,  23.233818  ,  43.530956  ,\n",
       "         68.86761   ,  12.159484  ,  42.34217   ,  54.943527  ,\n",
       "          5.344706  ,  41.65416   ,  43.040413  ,  10.437077  ,\n",
       "         41.128445  ,  32.20306   ,  17.98311   ,  40.29883   ,\n",
       "         19.966682  ,  24.278229  ,  39.657093  ,   9.352381  ,\n",
       "         30.136898  ,  41.438023  ,  -2.884353  ,  34.041355  ,\n",
       "         43.836857  , -15.287368  ,  37.65228   ,  43.541378  ,\n",
       "        -25.396862  ,  40.32444   ,  50.970123  , -31.854712  ,\n",
       "         36.370705  ,  50.96266   , -40.453888  ,  34.16218   ,\n",
       "         46.852497  , -38.145924  ,  25.732567  ,  35.649475  ,\n",
       "        -27.281328  ,  12.667004  ,  -5.496917  ,   2.946937  ,\n",
       "         -1.8291466 ,  -1.047319  ,   0.88303155,  -0.838334  ,\n",
       "          0.5851649 ,  -0.0396621 ,  -0.04110734,  -0.08073356,\n",
       "          0.06935974,  -0.6384577 ,  -0.6555605 ,   0.09905431,\n",
       "          0.0307072 ]], dtype=float32)"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = decoder.predict(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer, InputSpec\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         Measure the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_75 (InputLayer)        (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 64)                6016      \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 8)                 128       \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kmeans.fit_predict(encoder.predict(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('./label.csv',index=0 ,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
