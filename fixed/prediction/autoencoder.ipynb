{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/lab606a/.virtualenvs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lab606a/.virtualenvs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lab606a/.virtualenvs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lab606a/.virtualenvs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lab606a/.virtualenvs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lab606a/.virtualenvs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, LSTM, Activation, TimeDistributed, RepeatVector, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# set GPU memory\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 93)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/home/lab606a/ML/trajectories/fixed/classification/Lebeled_by_Kmeans/dataset20200320.csv', header=None)\n",
    "dataset = dataset.fillna(0)\n",
    "dataset = np.array(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 31, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.reshape(dataset.shape[0], int(dataset.shape[1]/3), 3)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_layer = Input(shape=(93,)) ##for Dense\n",
    "input_layer = Input(shape=(31,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = LSTM(64, activation='linear', return_sequences=True)(input_layer)\n",
    "#encoder = LSTM(128, activation='linear', return_sequences=True)(encoder)\n",
    "encoded = LSTM(16, activation='linear')(encoded)\n",
    "\n",
    "\n",
    "## Dense\n",
    "#encoded = Dense(64, activation='linear')(input_layer)\n",
    "#encoded = Dense(16, activation='linear')(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = RepeatVector(dataset.shape[1])(encoded)\n",
    "#decoded = LSTM(16, activation='linear', return_sequences=True)(decoded)\n",
    "decoded = LSTM(64, activation='linear', return_sequences=True)(decoded)\n",
    "#decoder = LSTM(3, activation='linear', return_sequences=True)(decoder)\n",
    "decoded = TimeDistributed(Dense(3))(decoded)\n",
    "\n",
    "## Dense\n",
    "#decoded = Dense(64, activation='linear')(encoded)\n",
    "#decoded = Dense(93, activation='linear')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_layer, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 31, 3)             0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 31, 64)            17408     \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 16)                5184      \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 31, 64)            20736     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 31, 3)             195       \n",
      "=================================================================\n",
      "Total params: 43,523\n",
      "Trainable params: 43,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AutoEncoder = Model(input_layer, decoded)\n",
    "AutoEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lstm_17/TensorArrayReadV3:0' shape=(?, 16) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoEncoder.layers[2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_input = Input(shape=(16,))\n",
    "decoded_layer = AutoEncoder.layers[3](decoded_input)\n",
    "decoded_layer = AutoEncoder.layers[4](decoded_layer)\n",
    "decoder = Model(decoded_input, decoded_layer)\n",
    "\n",
    "## Dense\n",
    "#decoded_input = Input(shape=(16,))\n",
    "#decoded_layer = AutoEncoder.layers[3](decoded_input)\n",
    "#decoded_layer = AutoEncoder.layers[4](decoded_layer)\n",
    "#decoder = Model(decoded_input, decoded_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder = Model(input_layer, encoder)\n",
    "#Encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4000/4000 [==============================] - 2s 524us/step - loss: 2242.3607 - acc: 0.3570\n",
      "Epoch 2/200\n",
      "4000/4000 [==============================] - 1s 173us/step - loss: 1853.3851 - acc: 0.3198\n",
      "Epoch 3/200\n",
      "4000/4000 [==============================] - 1s 174us/step - loss: 1779.3619 - acc: 0.2920\n",
      "Epoch 4/200\n",
      "4000/4000 [==============================] - 1s 175us/step - loss: 1448.3750 - acc: 0.2982\n",
      "Epoch 5/200\n",
      "4000/4000 [==============================] - 1s 174us/step - loss: 1587.8602 - acc: 0.2872\n",
      "Epoch 6/200\n",
      "4000/4000 [==============================] - 1s 177us/step - loss: 1530.1342 - acc: 0.2763\n",
      "Epoch 7/200\n",
      "4000/4000 [==============================] - 1s 174us/step - loss: 1156.1580 - acc: 0.2815\n",
      "Epoch 8/200\n",
      "4000/4000 [==============================] - 1s 175us/step - loss: 895.8396 - acc: 0.3087\n",
      "Epoch 9/200\n",
      "4000/4000 [==============================] - 1s 179us/step - loss: 899.4442 - acc: 0.3280\n",
      "Epoch 10/200\n",
      "4000/4000 [==============================] - 1s 176us/step - loss: 780.5267 - acc: 0.3178\n",
      "Epoch 11/200\n",
      "4000/4000 [==============================] - 1s 180us/step - loss: 659.6237 - acc: 0.3175\n",
      "Epoch 12/200\n",
      "4000/4000 [==============================] - 1s 179us/step - loss: 549.0057 - acc: 0.3108\n",
      "Epoch 13/200\n",
      "4000/4000 [==============================] - 1s 179us/step - loss: 628.0522 - acc: 0.3158\n",
      "Epoch 14/200\n",
      "4000/4000 [==============================] - 1s 178us/step - loss: 956.7773 - acc: 0.3009\n",
      "Epoch 15/200\n",
      "4000/4000 [==============================] - 1s 182us/step - loss: 719.7168 - acc: 0.3030\n",
      "Epoch 16/200\n",
      "4000/4000 [==============================] - 1s 180us/step - loss: 737.6179 - acc: 0.2962\n",
      "Epoch 17/200\n",
      "4000/4000 [==============================] - 1s 176us/step - loss: 686.2858 - acc: 0.2893\n",
      "Epoch 18/200\n",
      "4000/4000 [==============================] - 1s 171us/step - loss: 495.2122 - acc: 0.2972\n",
      "Epoch 19/200\n",
      "4000/4000 [==============================] - 1s 182us/step - loss: 441.9785 - acc: 0.3014\n",
      "Epoch 20/200\n",
      "4000/4000 [==============================] - 1s 178us/step - loss: 383.7984 - acc: 0.3056\n",
      "Epoch 21/200\n",
      "4000/4000 [==============================] - 1s 181us/step - loss: 372.6702 - acc: 0.3085\n",
      "Epoch 22/200\n",
      "4000/4000 [==============================] - 1s 353us/step - loss: 341.0321 - acc: 0.3381\n",
      "Epoch 23/200\n",
      "4000/4000 [==============================] - 4s 989us/step - loss: 318.6733 - acc: 0.3514\n",
      "Epoch 24/200\n",
      "4000/4000 [==============================] - 4s 994us/step - loss: 243.3037 - acc: 0.3553\n",
      "Epoch 25/200\n",
      "4000/4000 [==============================] - 4s 986us/step - loss: 231.3573 - acc: 0.3452\n",
      "Epoch 26/200\n",
      "4000/4000 [==============================] - 4s 971us/step - loss: 249.0847 - acc: 0.3335\n",
      "Epoch 27/200\n",
      "4000/4000 [==============================] - 4s 969us/step - loss: 279.0047 - acc: 0.3427\n",
      "Epoch 28/200\n",
      "4000/4000 [==============================] - 4s 950us/step - loss: 272.4980 - acc: 0.3525\n",
      "Epoch 29/200\n",
      "4000/4000 [==============================] - 4s 955us/step - loss: 236.4394 - acc: 0.3620\n",
      "Epoch 30/200\n",
      "4000/4000 [==============================] - 4s 981us/step - loss: 236.3723 - acc: 0.3569\n",
      "Epoch 31/200\n",
      "4000/4000 [==============================] - 4s 966us/step - loss: 177.8269 - acc: 0.3518\n",
      "Epoch 32/200\n",
      "4000/4000 [==============================] - 4s 944us/step - loss: 123.3697 - acc: 0.3353\n",
      "Epoch 33/200\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 96.5628 - acc: 0.3420\n",
      "Epoch 34/200\n",
      "4000/4000 [==============================] - 4s 974us/step - loss: 92.5068 - acc: 0.3608\n",
      "Epoch 35/200\n",
      "4000/4000 [==============================] - 4s 970us/step - loss: 86.4663 - acc: 0.3549\n",
      "Epoch 36/200\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 85.4031 - acc: 0.3591\n",
      "Epoch 37/200\n",
      "4000/4000 [==============================] - 4s 979us/step - loss: 78.7883 - acc: 0.3489\n",
      "Epoch 38/200\n",
      "4000/4000 [==============================] - 4s 958us/step - loss: 76.8903 - acc: 0.3441\n",
      "Epoch 39/200\n",
      "4000/4000 [==============================] - 4s 947us/step - loss: 71.6166 - acc: 0.3507\n",
      "Epoch 40/200\n",
      "4000/4000 [==============================] - 4s 963us/step - loss: 70.6909 - acc: 0.3470\n",
      "Epoch 41/200\n",
      "4000/4000 [==============================] - 4s 989us/step - loss: 72.4194 - acc: 0.3582\n",
      "Epoch 42/200\n",
      "4000/4000 [==============================] - 4s 972us/step - loss: 69.7046 - acc: 0.3721\n",
      "Epoch 43/200\n",
      "4000/4000 [==============================] - 4s 961us/step - loss: 67.5169 - acc: 0.3768\n",
      "Epoch 44/200\n",
      "4000/4000 [==============================] - 4s 984us/step - loss: 65.6939 - acc: 0.3929\n",
      "Epoch 45/200\n",
      "4000/4000 [==============================] - 4s 975us/step - loss: 65.8824 - acc: 0.3891\n",
      "Epoch 46/200\n",
      "4000/4000 [==============================] - 4s 976us/step - loss: 70.7277 - acc: 0.3921\n",
      "Epoch 47/200\n",
      "4000/4000 [==============================] - 4s 967us/step - loss: 73.4382 - acc: 0.3955\n",
      "Epoch 48/200\n",
      "4000/4000 [==============================] - 4s 984us/step - loss: 71.8483 - acc: 0.3966\n",
      "Epoch 49/200\n",
      "4000/4000 [==============================] - 4s 972us/step - loss: 72.1089 - acc: 0.4006\n",
      "Epoch 50/200\n",
      "4000/4000 [==============================] - 4s 984us/step - loss: 74.1098 - acc: 0.4017\n",
      "Epoch 51/200\n",
      "4000/4000 [==============================] - 4s 990us/step - loss: 73.1712 - acc: 0.4003\n",
      "Epoch 52/200\n",
      "4000/4000 [==============================] - 4s 980us/step - loss: 72.2029 - acc: 0.3784\n",
      "Epoch 53/200\n",
      "4000/4000 [==============================] - 4s 982us/step - loss: 69.1125 - acc: 0.3909\n",
      "Epoch 54/200\n",
      "4000/4000 [==============================] - 4s 968us/step - loss: 70.1102 - acc: 0.3899\n",
      "Epoch 55/200\n",
      "4000/4000 [==============================] - 4s 966us/step - loss: 68.4492 - acc: 0.3923\n",
      "Epoch 56/200\n",
      "4000/4000 [==============================] - 4s 960us/step - loss: 71.7294 - acc: 0.3917\n",
      "Epoch 57/200\n",
      "4000/4000 [==============================] - 4s 958us/step - loss: 71.9385 - acc: 0.3932\n",
      "Epoch 58/200\n",
      "4000/4000 [==============================] - 4s 953us/step - loss: 70.9612 - acc: 0.3956\n",
      "Epoch 59/200\n",
      "4000/4000 [==============================] - 4s 966us/step - loss: 70.2933 - acc: 0.4058\n",
      "Epoch 60/200\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 71.5106 - acc: 0.3838\n",
      "Epoch 61/200\n",
      "4000/4000 [==============================] - 4s 971us/step - loss: 69.0006 - acc: 0.3960\n",
      "Epoch 62/200\n",
      "4000/4000 [==============================] - 4s 991us/step - loss: 68.4733 - acc: 0.3933\n",
      "Epoch 63/200\n",
      "4000/4000 [==============================] - 4s 988us/step - loss: 67.7759 - acc: 0.3801\n",
      "Epoch 64/200\n",
      "4000/4000 [==============================] - 4s 987us/step - loss: 65.4312 - acc: 0.3913\n",
      "Epoch 65/200\n",
      "4000/4000 [==============================] - 4s 979us/step - loss: 63.8685 - acc: 0.3832\n",
      "Epoch 66/200\n",
      "4000/4000 [==============================] - 4s 972us/step - loss: 64.8086 - acc: 0.3904\n",
      "Epoch 67/200\n",
      "4000/4000 [==============================] - 4s 978us/step - loss: 64.2175 - acc: 0.3877\n",
      "Epoch 68/200\n",
      "4000/4000 [==============================] - 4s 958us/step - loss: 61.3074 - acc: 0.4084\n",
      "Epoch 69/200\n",
      "4000/4000 [==============================] - 4s 969us/step - loss: 61.8361 - acc: 0.4312\n",
      "Epoch 70/200\n",
      "4000/4000 [==============================] - 4s 980us/step - loss: 60.3163 - acc: 0.4527\n",
      "Epoch 71/200\n",
      "4000/4000 [==============================] - 4s 990us/step - loss: 59.1063 - acc: 0.4561\n",
      "Epoch 72/200\n",
      "4000/4000 [==============================] - 4s 950us/step - loss: 57.4798 - acc: 0.4508\n",
      "Epoch 73/200\n",
      "4000/4000 [==============================] - 4s 967us/step - loss: 56.0636 - acc: 0.4538\n",
      "Epoch 74/200\n",
      "4000/4000 [==============================] - 4s 969us/step - loss: 55.5849 - acc: 0.4606\n",
      "Epoch 75/200\n",
      "4000/4000 [==============================] - 4s 963us/step - loss: 55.5773 - acc: 0.4570\n",
      "Epoch 76/200\n",
      "4000/4000 [==============================] - 4s 968us/step - loss: 55.5615 - acc: 0.4679\n",
      "Epoch 77/200\n",
      "4000/4000 [==============================] - 4s 985us/step - loss: 54.8577 - acc: 0.4594\n",
      "Epoch 78/200\n",
      "4000/4000 [==============================] - 4s 982us/step - loss: 55.4149 - acc: 0.4761\n",
      "Epoch 79/200\n",
      "4000/4000 [==============================] - 4s 976us/step - loss: 54.3197 - acc: 0.4708\n",
      "Epoch 80/200\n",
      "4000/4000 [==============================] - 4s 975us/step - loss: 54.4204 - acc: 0.4790\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 4s 963us/step - loss: 54.3155 - acc: 0.4891\n",
      "Epoch 82/200\n",
      "4000/4000 [==============================] - 4s 973us/step - loss: 54.2698 - acc: 0.4522\n",
      "Epoch 83/200\n",
      "4000/4000 [==============================] - 4s 975us/step - loss: 54.7520 - acc: 0.4500\n",
      "Epoch 84/200\n",
      "4000/4000 [==============================] - 4s 985us/step - loss: 54.5626 - acc: 0.4604\n",
      "Epoch 85/200\n",
      "4000/4000 [==============================] - 4s 984us/step - loss: 54.5935 - acc: 0.4447\n",
      "Epoch 86/200\n",
      "4000/4000 [==============================] - 4s 971us/step - loss: 54.5258 - acc: 0.4564\n",
      "Epoch 87/200\n",
      "4000/4000 [==============================] - 4s 979us/step - loss: 54.3457 - acc: 0.4539\n",
      "Epoch 88/200\n",
      "4000/4000 [==============================] - 4s 958us/step - loss: 54.3989 - acc: 0.4552\n",
      "Epoch 89/200\n",
      "4000/4000 [==============================] - 4s 982us/step - loss: 54.3233 - acc: 0.4623\n",
      "Epoch 90/200\n",
      "4000/4000 [==============================] - 4s 970us/step - loss: 54.1494 - acc: 0.4666\n",
      "Epoch 91/200\n",
      "4000/4000 [==============================] - 4s 962us/step - loss: 54.2524 - acc: 0.4654\n",
      "Epoch 92/200\n",
      "4000/4000 [==============================] - 4s 966us/step - loss: 54.1916 - acc: 0.4434\n",
      "Epoch 93/200\n",
      "4000/4000 [==============================] - 4s 974us/step - loss: 54.0926 - acc: 0.4539\n",
      "Epoch 94/200\n",
      "4000/4000 [==============================] - 4s 974us/step - loss: 54.0404 - acc: 0.4541\n",
      "Epoch 95/200\n",
      "4000/4000 [==============================] - 4s 980us/step - loss: 54.0265 - acc: 0.4404\n",
      "Epoch 96/200\n",
      "4000/4000 [==============================] - 4s 968us/step - loss: 54.1380 - acc: 0.4230\n",
      "Epoch 97/200\n",
      "4000/4000 [==============================] - 4s 969us/step - loss: 54.1865 - acc: 0.4299\n",
      "Epoch 98/200\n",
      "4000/4000 [==============================] - 4s 962us/step - loss: 54.9110 - acc: 0.4580\n",
      "Epoch 99/200\n",
      "4000/4000 [==============================] - 4s 959us/step - loss: 54.6424 - acc: 0.4361\n",
      "Epoch 100/200\n",
      "4000/4000 [==============================] - 4s 967us/step - loss: 55.1396 - acc: 0.4318\n",
      "Epoch 101/200\n",
      "4000/4000 [==============================] - 4s 961us/step - loss: 55.0225 - acc: 0.4315\n",
      "Epoch 102/200\n",
      "4000/4000 [==============================] - 4s 971us/step - loss: 54.9376 - acc: 0.4383\n",
      "Epoch 103/200\n",
      "4000/4000 [==============================] - 4s 960us/step - loss: 54.8467 - acc: 0.4221\n",
      "Epoch 104/200\n",
      "4000/4000 [==============================] - 4s 973us/step - loss: 54.2244 - acc: 0.4277\n",
      "Epoch 105/200\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 54.0338 - acc: 0.4538\n",
      "Epoch 106/200\n",
      "4000/4000 [==============================] - 4s 997us/step - loss: 54.4223 - acc: 0.4578\n",
      "Epoch 107/200\n",
      "4000/4000 [==============================] - 4s 994us/step - loss: 54.8576 - acc: 0.4489\n",
      "Epoch 108/200\n",
      "4000/4000 [==============================] - 4s 993us/step - loss: 54.6267 - acc: 0.4462\n",
      "Epoch 109/200\n",
      "4000/4000 [==============================] - 4s 988us/step - loss: 54.4345 - acc: 0.4408\n",
      "Epoch 110/200\n",
      "4000/4000 [==============================] - 4s 969us/step - loss: 54.1373 - acc: 0.4427\n",
      "Epoch 111/200\n",
      "4000/4000 [==============================] - 4s 963us/step - loss: 54.0061 - acc: 0.4589\n",
      "Epoch 112/200\n",
      "4000/4000 [==============================] - 4s 968us/step - loss: 53.8374 - acc: 0.4496\n",
      "Epoch 113/200\n",
      "4000/4000 [==============================] - 4s 958us/step - loss: 53.6313 - acc: 0.4740\n",
      "Epoch 114/200\n",
      "4000/4000 [==============================] - 4s 966us/step - loss: 53.4847 - acc: 0.4612\n",
      "Epoch 115/200\n",
      "4000/4000 [==============================] - 4s 960us/step - loss: 53.4797 - acc: 0.4695\n",
      "Epoch 116/200\n",
      "4000/4000 [==============================] - 4s 980us/step - loss: 53.5217 - acc: 0.4719\n",
      "Epoch 117/200\n",
      "4000/4000 [==============================] - 4s 984us/step - loss: 53.6632 - acc: 0.4623\n",
      "Epoch 118/200\n",
      "4000/4000 [==============================] - 4s 980us/step - loss: 54.6556 - acc: 0.4442\n",
      "Epoch 119/200\n",
      "4000/4000 [==============================] - 4s 963us/step - loss: 54.3939 - acc: 0.4511\n",
      "Epoch 120/200\n",
      "4000/4000 [==============================] - 4s 967us/step - loss: 54.6126 - acc: 0.4247\n",
      "Epoch 121/200\n",
      "4000/4000 [==============================] - 4s 974us/step - loss: 53.5982 - acc: 0.4378\n",
      "Epoch 122/200\n",
      "4000/4000 [==============================] - 4s 964us/step - loss: 53.8044 - acc: 0.4514\n",
      "Epoch 123/200\n",
      "4000/4000 [==============================] - 4s 977us/step - loss: 54.2704 - acc: 0.4396\n",
      "Epoch 124/200\n",
      "4000/4000 [==============================] - 4s 985us/step - loss: 54.4632 - acc: 0.4437\n",
      "Epoch 125/200\n",
      "4000/4000 [==============================] - 4s 974us/step - loss: 53.9264 - acc: 0.4318\n",
      "Epoch 126/200\n",
      "4000/4000 [==============================] - 4s 981us/step - loss: 53.2208 - acc: 0.4672\n",
      "Epoch 127/200\n",
      "4000/4000 [==============================] - 4s 974us/step - loss: 53.7225 - acc: 0.4511\n",
      "Epoch 128/200\n",
      "4000/4000 [==============================] - 4s 972us/step - loss: 53.9595 - acc: 0.4444\n",
      "Epoch 129/200\n",
      "4000/4000 [==============================] - 4s 967us/step - loss: 53.9730 - acc: 0.4497\n",
      "Epoch 130/200\n",
      "4000/4000 [==============================] - 4s 962us/step - loss: 54.5882 - acc: 0.4665\n",
      "Epoch 131/200\n",
      "4000/4000 [==============================] - 4s 961us/step - loss: 53.6896 - acc: 0.4547\n",
      "Epoch 132/200\n",
      "4000/4000 [==============================] - 4s 965us/step - loss: 53.2779 - acc: 0.4563\n",
      "Epoch 133/200\n",
      "4000/4000 [==============================] - 4s 966us/step - loss: 53.4975 - acc: 0.4499\n",
      "Epoch 134/200\n",
      "4000/4000 [==============================] - 4s 963us/step - loss: 53.3166 - acc: 0.4638\n",
      "Epoch 135/200\n",
      "4000/4000 [==============================] - 4s 961us/step - loss: 53.1686 - acc: 0.4625\n",
      "Epoch 136/200\n",
      "4000/4000 [==============================] - 4s 963us/step - loss: 53.2660 - acc: 0.4749\n",
      "Epoch 137/200\n",
      "4000/4000 [==============================] - 4s 957us/step - loss: 53.0236 - acc: 0.4693\n",
      "Epoch 138/200\n",
      "4000/4000 [==============================] - 4s 971us/step - loss: 52.4761 - acc: 0.4872\n",
      "Epoch 139/200\n",
      "4000/4000 [==============================] - 4s 962us/step - loss: 52.3130 - acc: 0.4675\n",
      "Epoch 140/200\n",
      "4000/4000 [==============================] - 4s 966us/step - loss: 52.6606 - acc: 0.4690\n",
      "Epoch 141/200\n",
      "4000/4000 [==============================] - 4s 958us/step - loss: 52.7333 - acc: 0.4676\n",
      "Epoch 142/200\n",
      "4000/4000 [==============================] - 4s 970us/step - loss: 53.1294 - acc: 0.4560\n",
      "Epoch 143/200\n",
      "4000/4000 [==============================] - 4s 960us/step - loss: 52.7325 - acc: 0.4663\n",
      "Epoch 144/200\n",
      "4000/4000 [==============================] - 4s 966us/step - loss: 53.4104 - acc: 0.4476\n",
      "Epoch 145/200\n",
      "4000/4000 [==============================] - 4s 959us/step - loss: 53.5336 - acc: 0.4647\n",
      "Epoch 146/200\n",
      "4000/4000 [==============================] - 4s 954us/step - loss: 53.6981 - acc: 0.4525\n",
      "Epoch 147/200\n",
      "4000/4000 [==============================] - 4s 960us/step - loss: 53.6499 - acc: 0.4373\n",
      "Epoch 148/200\n",
      "4000/4000 [==============================] - 4s 971us/step - loss: 53.6584 - acc: 0.4286\n",
      "Epoch 149/200\n",
      "4000/4000 [==============================] - 4s 972us/step - loss: 53.6374 - acc: 0.4281\n",
      "Epoch 150/200\n",
      "4000/4000 [==============================] - 4s 971us/step - loss: 53.5694 - acc: 0.4382\n",
      "Epoch 151/200\n",
      "4000/4000 [==============================] - 4s 969us/step - loss: 53.9505 - acc: 0.4360\n",
      "Epoch 152/200\n",
      "4000/4000 [==============================] - 4s 967us/step - loss: 54.7740 - acc: 0.4111\n",
      "Epoch 153/200\n",
      "4000/4000 [==============================] - 4s 966us/step - loss: 54.4521 - acc: 0.4318\n",
      "Epoch 154/200\n",
      "4000/4000 [==============================] - 4s 965us/step - loss: 53.7304 - acc: 0.4445\n",
      "Epoch 155/200\n",
      "4000/4000 [==============================] - 4s 962us/step - loss: 53.1527 - acc: 0.4576\n",
      "Epoch 156/200\n",
      "4000/4000 [==============================] - 4s 955us/step - loss: 53.1724 - acc: 0.4473\n",
      "Epoch 157/200\n",
      "4000/4000 [==============================] - 4s 961us/step - loss: 53.4210 - acc: 0.4544\n",
      "Epoch 158/200\n",
      "4000/4000 [==============================] - 4s 969us/step - loss: 53.1570 - acc: 0.4558\n",
      "Epoch 159/200\n",
      "4000/4000 [==============================] - 4s 969us/step - loss: 53.0166 - acc: 0.4451\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 4s 965us/step - loss: 52.6949 - acc: 0.4607\n",
      "Epoch 161/200\n",
      "4000/4000 [==============================] - 4s 961us/step - loss: 52.9580 - acc: 0.4512\n",
      "Epoch 162/200\n",
      "4000/4000 [==============================] - 4s 962us/step - loss: 53.2301 - acc: 0.4579\n",
      "Epoch 163/200\n",
      "4000/4000 [==============================] - 4s 968us/step - loss: 52.9752 - acc: 0.4610\n",
      "Epoch 164/200\n",
      "4000/4000 [==============================] - 4s 967us/step - loss: 52.8698 - acc: 0.4755\n",
      "Epoch 165/200\n",
      "4000/4000 [==============================] - 4s 971us/step - loss: 52.8154 - acc: 0.4690\n",
      "Epoch 166/200\n",
      "4000/4000 [==============================] - 4s 963us/step - loss: 52.8220 - acc: 0.4761\n",
      "Epoch 167/200\n",
      "4000/4000 [==============================] - 4s 967us/step - loss: 52.7271 - acc: 0.4666\n",
      "Epoch 168/200\n",
      "4000/4000 [==============================] - 4s 981us/step - loss: 52.7145 - acc: 0.4706\n",
      "Epoch 169/200\n",
      "4000/4000 [==============================] - 4s 958us/step - loss: 52.7088 - acc: 0.4495\n",
      "Epoch 170/200\n",
      "4000/4000 [==============================] - 4s 962us/step - loss: 52.6913 - acc: 0.4629\n",
      "Epoch 171/200\n",
      "4000/4000 [==============================] - 4s 985us/step - loss: 52.8068 - acc: 0.4525\n",
      "Epoch 172/200\n",
      "4000/4000 [==============================] - 4s 992us/step - loss: 52.8233 - acc: 0.4571\n",
      "Epoch 173/200\n",
      "4000/4000 [==============================] - 4s 974us/step - loss: 52.9473 - acc: 0.4484\n",
      "Epoch 174/200\n",
      "4000/4000 [==============================] - 4s 965us/step - loss: 53.0069 - acc: 0.4517\n",
      "Epoch 175/200\n",
      "4000/4000 [==============================] - 4s 985us/step - loss: 53.0055 - acc: 0.4328\n",
      "Epoch 176/200\n",
      "4000/4000 [==============================] - 4s 966us/step - loss: 53.0598 - acc: 0.4193\n",
      "Epoch 177/200\n",
      "4000/4000 [==============================] - 4s 968us/step - loss: 52.9886 - acc: 0.4300\n",
      "Epoch 178/200\n",
      "4000/4000 [==============================] - 4s 965us/step - loss: 52.9330 - acc: 0.4321\n",
      "Epoch 179/200\n",
      "4000/4000 [==============================] - 4s 976us/step - loss: 52.9244 - acc: 0.4318\n",
      "Epoch 180/200\n",
      "4000/4000 [==============================] - 4s 992us/step - loss: 52.8136 - acc: 0.4358\n",
      "Epoch 181/200\n",
      "4000/4000 [==============================] - 4s 968us/step - loss: 52.9069 - acc: 0.4409\n",
      "Epoch 182/200\n",
      "4000/4000 [==============================] - 4s 990us/step - loss: 52.9501 - acc: 0.4445\n",
      "Epoch 183/200\n",
      "4000/4000 [==============================] - 4s 963us/step - loss: 52.7877 - acc: 0.4471\n",
      "Epoch 184/200\n",
      "4000/4000 [==============================] - 4s 967us/step - loss: 52.8887 - acc: 0.4422\n",
      "Epoch 185/200\n",
      "4000/4000 [==============================] - 4s 961us/step - loss: 52.8984 - acc: 0.4564\n",
      "Epoch 186/200\n",
      "4000/4000 [==============================] - 4s 972us/step - loss: 52.8669 - acc: 0.4529\n",
      "Epoch 187/200\n",
      "4000/4000 [==============================] - 4s 971us/step - loss: 52.8607 - acc: 0.4595\n",
      "Epoch 188/200\n",
      "4000/4000 [==============================] - 4s 960us/step - loss: 52.9396 - acc: 0.4615\n",
      "Epoch 189/200\n",
      "4000/4000 [==============================] - 4s 968us/step - loss: 52.8261 - acc: 0.4578\n",
      "Epoch 190/200\n",
      "4000/4000 [==============================] - 4s 969us/step - loss: 52.8439 - acc: 0.4555\n",
      "Epoch 191/200\n",
      "4000/4000 [==============================] - 4s 962us/step - loss: 52.8745 - acc: 0.4515\n",
      "Epoch 192/200\n",
      "4000/4000 [==============================] - 4s 970us/step - loss: 52.6804 - acc: 0.4507\n",
      "Epoch 193/200\n",
      "4000/4000 [==============================] - 4s 969us/step - loss: 52.8266 - acc: 0.4395\n",
      "Epoch 194/200\n",
      "4000/4000 [==============================] - 4s 961us/step - loss: 52.8275 - acc: 0.4436\n",
      "Epoch 195/200\n",
      "4000/4000 [==============================] - 4s 973us/step - loss: 52.7745 - acc: 0.4394\n",
      "Epoch 196/200\n",
      "4000/4000 [==============================] - 4s 965us/step - loss: 52.7927 - acc: 0.4340\n",
      "Epoch 197/200\n",
      "4000/4000 [==============================] - 4s 960us/step - loss: 52.7258 - acc: 0.4380\n",
      "Epoch 198/200\n",
      "4000/4000 [==============================] - 4s 972us/step - loss: 52.7060 - acc: 0.4372\n",
      "Epoch 199/200\n",
      "4000/4000 [==============================] - 4s 982us/step - loss: 52.6731 - acc: 0.4328\n",
      "Epoch 200/200\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 52.6804 - acc: 0.4340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4691a972e8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoEncoder.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "AutoEncoder.fit(dataset, dataset, batch_size=400, epochs=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 93 into shape (1,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f9ce8147ed0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 93 into shape (1,1)"
     ]
    }
   ],
   "source": [
    "x_test = dataset[2,:]\n",
    "x_test = x_test.reshape(1,x_test.shape[0],x_test.shape[1])\n",
    "x_test = x_test.reshape(1, x_test.shape[0])\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AutoEncoder.predict(x_test)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = encoder.predict(x_test)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = decoder.predict(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer, InputSpec\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         Measure the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-917ffa5d0322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kmeans.fit_predict(encoder.predict(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('./label.csv',index=0 ,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
